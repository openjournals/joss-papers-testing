<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">0</article-id>
<article-id pub-id-type="doi">N/A</article-id>
<title-group>
<article-title>Octopus-sensing: A python library for human behavior
studies</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-6082-9772</contrib-id>
<name>
<surname>Saffaryazdi</surname>
<given-names>Nastaran</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-6482-3944</contrib-id>
<name>
<surname>Gharibnavaz</surname>
<given-names>Aidin</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-4172-6759</contrib-id>
<name>
<surname>Billinghurst</surname>
<given-names>Mark</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Empathic Computing Laboratory, Auckland Bioengineering
Institute, University of Auckland</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Empathic Computing Laboratory, University of South
Australia</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Independent Researcher</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2021-12-01">
<day>1</day>
<month>12</month>
<year>2021</year>
</pub-date>
<volume>¿VOL?</volume>
<issue>¿ISSUE?</issue>
<fpage>¿PAGE?</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>Javascript</kwd>
<kwd>Human-Computer-Interaction(HCI)</kwd>
<kwd>Human behavior research</kwd>
<kwd>Physiological Signals</kwd>
<kwd>Electroencephalography</kwd>
<kwd>Multimodal Sensors</kwd>
<kwd>Synchronous data acquisition</kwd>
<kwd>Data visuaization</kwd>
<kwd>Affective Computing</kwd>
<kwd>Experimental design</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Designing user studies and collecting data is critical in exploring
  and automatically recognizing human behavior. Currently, it is
  possible to use a wide range of sensors to capture heart rate, brain
  activity, skin conductance, and a variety of different physiological
  cues
  (<xref alt="Seneviratne et al., 2017" rid="ref-seneviratne2017survey" ref-type="bibr">Seneviratne
  et al., 2017</xref>). These can be combined together to provide
  information about a user’s emotional state
  (<xref alt="Dzedzickis et al., 2020" rid="ref-dzedzickis2020human" ref-type="bibr">Dzedzickis
  et al., 2020</xref>;
  <xref alt="Egger et al., 2019" rid="ref-egger2019emotion" ref-type="bibr">Egger
  et al., 2019</xref>), cognitive load
  (<xref alt="Mangaroska et al., 2021" rid="ref-mangaroska2021exploring" ref-type="bibr">Mangaroska
  et al., 2021</xref>;
  <xref alt="Vanneste et al., 2020" rid="ref-vanneste2020towards" ref-type="bibr">Vanneste
  et al., 2020</xref>), or other factors. However, even when data are
  collected correctly, synchronizing data from multiple sensors is
  time-consuming and subject to human error. Failure to record and
  synchronize data can lead to incorrect analysis and results, and
  finally, the need to repeat the time-consuming experiments several
  times.</p>
  <p>To overcome these challenges,
  <monospace>Octopus Sensing</monospace> facilitates synchronous data
  acquisition from various sources and provides some utilities for
  designing user studies, real-time monitoring, and offline data
  visualization. The major aim of
  <monospace>Octopus Sensing</monospace>is to provide a simple scripting
  interface so that people with little or no software development skills
  can define sensor-based experiment scenarios with minimum effort.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>External events affect the human body and mind, creating many
  internal and external changes in response to external stimuli.
  Nowadays, researchers use various sensors to monitor and measure these
  responses to know more about a person’s state
  (<xref alt="Chen et al., 2020" rid="ref-chen2020physiological" ref-type="bibr">Chen
  et al., 2020</xref>;
  <xref alt="Kreibig, 2010" rid="ref-kreibig2010autonomic" ref-type="bibr">Kreibig,
  2010</xref>;
  <xref alt="Sun et al., 2020" rid="ref-sun2020multimodal" ref-type="bibr">Sun
  et al., 2020</xref>) and employ sensors in many healthcare
  applications like assisting patients or mental health monitoring
  (<xref alt="Hassouneh et al., 2020" rid="ref-hassouneh2020development" ref-type="bibr">Hassouneh
  et al., 2020</xref>). They can also be used to improve social
  interactions
  (<xref alt="Hossain &amp; Gedeon, 2019" rid="ref-hossain2019observers" ref-type="bibr">Hossain
  &amp; Gedeon, 2019</xref>;
  <xref alt="Verschuere et al., 2006" rid="ref-verschuere2006psychopathy" ref-type="bibr">Verschuere
  et al., 2006</xref>), and creating better quality of life by making
  intelligent devices like Intelligent Tutoring Systems
  (<xref alt="Dewan et al., 2019" rid="ref-dewan2019engagement" ref-type="bibr">Dewan
  et al., 2019</xref>) or making interactive robots and virtual
  characters
  (<xref alt="Hong et al., 2020" rid="ref-hong2020multimodal" ref-type="bibr">Hong
  et al., 2020</xref>;
  <xref alt="Val-Calvo et al., 2020" rid="ref-val2020affective" ref-type="bibr">Val-Calvo
  et al., 2020</xref>).</p>
  <p>Recently, many researchers have tried to achieve a deeper
  understanding of humans by simultaneously interpreting a combination
  of physiological and behavioral changes in the human body
  (<xref alt="Koelstra et al., 2011" rid="ref-koelstra2011deap" ref-type="bibr">Koelstra
  et al., 2011</xref>;
  <xref alt="Shu et al., 2018" rid="ref-shu2018review" ref-type="bibr">Shu
  et al., 2018</xref>). Acquiring and analyzing data from different
  sources with various hardware and software is complex, time-consuming,
  and challenging. Also, synchronously recording data with multiple
  formats can be easily affected by human error. These tasks slow down
  the pace of progress in human-computer interaction and human behavior
  research.</p>
  <p>There are a limited number of frameworks that support synchronous
  data acquisition. For example,
  <ext-link ext-link-type="uri" xlink:href="https://imotions.com/">iMotions</ext-link>
  has developed software that integrates and synchronizes a wide range
  of various sensors and devices that record a considerable range of
  signals synchronously. Although iMotions offers many useful features,
  it is a commercial software and is not open-source.</p>
  <p><monospace>Octopus Sensing</monospace> is a lightweight open-source
  multi-platform library that facilitates synchronous data acquisition
  from various sources and can be extended to process and analyze data
  in real-time. It provides a web-based real-time monitoring system that
  can be used remotely to illustrate and monitor signals in real-time.
  It also provides offline data visualization to see the changes in
  various sensors at the same time.</p>
</sec>
<sec id="overview">
  <title>Overview</title>
  <p>Octopus Sensing is a tool to help in running scientific experiments
  that involve recording data synchronously from multiple sources. It
  can simultaneously collect data from various devices such as
  <ext-link ext-link-type="uri" xlink:href="https://openbci.com/">OpenBCI
  EEG headset</ext-link>,
  <ext-link ext-link-type="uri" xlink:href="https://shimmersensing.com">Shimmer3
  sensor</ext-link>, camera and audio-recorder. Data collection can be
  started and stopped synchronously across all devices.</p>
  <p>The main features of <monospace>Octopus Sensing</monospace> are
  listed as follows:</p>
  <list list-type="bullet">
    <list-item>
      <p>Manages data recording from multiple sources using a simple
      unified interface.</p>
    </list-item>
    <list-item>
      <p>Minimizes human errors from manipulating data in synchronous
      data collection.</p>
    </list-item>
    <list-item>
      <p>Provides some utilities for designing studies like showing
      different stimuli or designing questionnaires.</p>
    </list-item>
    <list-item>
      <p>Offers a monitoring interface that prepares and visualizes
      collected data in real-time.</p>
    </list-item>
    <list-item>
      <p>Provides offline visualization of data from multiple sources
      simultaneously.</p>
    </list-item>
  </list>
</sec>
<sec id="research-perspective">
  <title>Research perspective</title>
  <p>We have used <monospace>Octopus Sensing</monospace> in designing
  several experiments in human emotion recognition. We designed the
  experiments and recorded facial video, brain activity, and
  physiological signals using <monospace>Octopus Sensing</monospace> in
  a watching video task to recognize emotion
  (<xref alt="S. N. Nastaran Saffaryazdi Syed Talal Wasim &amp; Billinghurst, n.d." rid="ref-Saffaryazdi2021" ref-type="bibr">S.
  N. Nastaran Saffaryazdi Syed Talal Wasim &amp; Billinghurst,
  n.d.</xref>). This scenario which is common in physiological emotion
  recognition studies, has been included in the repository as an example
  and explained in the tutorial. In another research study, we collected
  multimodal data in a face-to-face conversation task to analyze human
  emotional responses
  (<xref alt="N. S. Nastaran Saffaryazdi Yenushka Goonesekera et al., n.d." rid="ref-Saffaryazdi2021conv" ref-type="bibr">N.
  S. Nastaran Saffaryazdi Yenushka Goonesekera et al., n.d.</xref>).</p>
  <p>This tool can be used as the base structure for creating real-time
  data processing systems with capabilities to recognize emotions,
  stress, cognitive load, or analyze human behaviors. In the future, we
  want to extend its features to provide real-time emotion recognition
  using multimodal data analysis.</p>
</sec>
<sec id="acknowledgement">
  <title>Acknowledgement</title>
  <p>We acknowledge the
  <ext-link ext-link-type="uri" xlink:href="http://empathiccomputing.org/">Empatic
  Computing Laboratory</ext-link> for financial support and providing
  feedback.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-kreibig2010autonomic">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Kreibig</surname><given-names>Sylvia D</given-names></name>
        </person-group>
        <article-title>Autonomic nervous system activity in emotion: A review</article-title>
        <source>Biological psychology</source>
        <publisher-name>Elsevier</publisher-name>
        <year iso-8601-date="2010">2010</year>
        <volume>84</volume>
        <issue>3</issue>
        <pub-id pub-id-type="doi">10.1016/j.biopsycho.2010.03.010</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-chen2020physiological">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Chen</surname><given-names>Kuan-Hua</given-names></name>
          <name><surname>Brown</surname><given-names>Casey L</given-names></name>
          <name><surname>Wells</surname><given-names>Jenna L</given-names></name>
          <name><surname>Rothwell</surname><given-names>Emily S</given-names></name>
          <name><surname>Otero</surname><given-names>Marcela C</given-names></name>
          <name><surname>Levenson</surname><given-names>Robert W</given-names></name>
          <name><surname>Fredrickson</surname><given-names>Barbara L</given-names></name>
        </person-group>
        <article-title>Physiological linkage during shared positive and shared negative emotion.</article-title>
        <source>Journal of Personality and Social Psychology</source>
        <publisher-name>American Psychological Association</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <pub-id pub-id-type="doi">10.1037/pspi0000337</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-sun2020multimodal">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Sun</surname><given-names>Yanjia</given-names></name>
          <name><surname>Ayaz</surname><given-names>Hasan</given-names></name>
          <name><surname>Akansu</surname><given-names>Ali N</given-names></name>
        </person-group>
        <article-title>Multimodal affective state assessment using fNIRS+ EEG and spontaneous facial expression</article-title>
        <source>Brain Sciences</source>
        <publisher-name>Multidisciplinary Digital Publishing Institute</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <volume>10</volume>
        <issue>2</issue>
        <pub-id pub-id-type="doi">10.3390/brainsci10020085</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-hassouneh2020development">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Hassouneh</surname><given-names>Aya</given-names></name>
          <name><surname>Mutawa</surname><given-names>AM</given-names></name>
          <name><surname>Murugappan</surname><given-names>M</given-names></name>
        </person-group>
        <article-title>Development of a real-time emotion recognition system using facial expressions and EEG based on machine learning and deep neural network methods</article-title>
        <source>Informatics in Medicine Unlocked</source>
        <publisher-name>Elsevier</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <pub-id pub-id-type="doi">10.1016/j.imu.2020.100372</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-verschuere2006psychopathy">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Verschuere</surname><given-names>Bruno</given-names></name>
          <name><surname>Crombez</surname><given-names>Geert</given-names></name>
          <name><surname>Koster</surname><given-names>Ernst</given-names></name>
          <name><surname>Uzieblo</surname><given-names>Katarzyna</given-names></name>
        </person-group>
        <article-title>Psychopathy and physiological detection of concealed information: A review</article-title>
        <source>Psychologica Belgica</source>
        <publisher-name>Ubiquity Press</publisher-name>
        <year iso-8601-date="2006">2006</year>
        <volume>46</volume>
        <issue>1-2</issue>
        <pub-id pub-id-type="doi">10.5334/pb-46-1-2-99</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-hossain2019observers">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Hossain</surname><given-names>Md Zakir</given-names></name>
          <name><surname>Gedeon</surname><given-names>Tom</given-names></name>
        </person-group>
        <article-title>Observers’ physiological measures in response to videos can be used to detect genuine smiles</article-title>
        <source>International Journal of Human-Computer Studies</source>
        <publisher-name>Elsevier</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <volume>122</volume>
        <pub-id pub-id-type="doi">10.1016/j.ijhcs.2018.10.003</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-dewan2019engagement">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Dewan</surname><given-names>M Ali Akber</given-names></name>
          <name><surname>Murshed</surname><given-names>Mahbub</given-names></name>
          <name><surname>Lin</surname><given-names>Fuhua</given-names></name>
        </person-group>
        <article-title>Engagement detection in online learning: A review</article-title>
        <source>Smart Learning Environments</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <volume>6</volume>
        <issue>1</issue>
        <pub-id pub-id-type="doi">10.1186/s40561-018-0080-z</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-val2020affective">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Val-Calvo</surname><given-names>Mikel</given-names></name>
          <name><surname>Álvarez-Sánchez</surname><given-names>José Ramón</given-names></name>
          <name><surname>Ferrández-Vicente</surname><given-names>José Manuel</given-names></name>
          <name><surname>Fernández</surname><given-names>Eduardo</given-names></name>
        </person-group>
        <article-title>Affective robot story-telling human-robot interaction: Exploratory real-time emotion estimation analysis using facial expressions and physiological signals</article-title>
        <source>IEEE Access</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <volume>8</volume>
        <pub-id pub-id-type="doi">10.1109/ACCESS.2020.3007109</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-hong2020multimodal">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Hong</surname><given-names>Alexander</given-names></name>
          <name><surname>Lunscher</surname><given-names>Nolan</given-names></name>
          <name><surname>Hu</surname><given-names>Tianhao</given-names></name>
          <name><surname>Tsuboi</surname><given-names>Yuma</given-names></name>
          <name><surname>Zhang</surname><given-names>Xinyi</given-names></name>
          <name><surname>Reis Alves</surname><given-names>Silas Franco dos</given-names></name>
          <name><surname>Nejat</surname><given-names>Goldie</given-names></name>
          <name><surname>Benhabib</surname><given-names>Beno</given-names></name>
        </person-group>
        <article-title>A multimodal emotional human-robot interaction architecture for social robots engaged in bidirectional communication\backslashvspace* 7pt</article-title>
        <source>IEEE Transactions on Cybernetics</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <pub-id pub-id-type="doi">10.1109/TCYB.2020.2974688</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-shu2018review">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Shu</surname><given-names>Lin</given-names></name>
          <name><surname>Xie</surname><given-names>Jinyan</given-names></name>
          <name><surname>Yang</surname><given-names>Mingyue</given-names></name>
          <name><surname>Li</surname><given-names>Ziyi</given-names></name>
          <name><surname>Li</surname><given-names>Zhenqi</given-names></name>
          <name><surname>Liao</surname><given-names>Dan</given-names></name>
          <name><surname>Xu</surname><given-names>Xiangmin</given-names></name>
          <name><surname>Yang</surname><given-names>Xinyi</given-names></name>
        </person-group>
        <article-title>A review of emotion recognition using physiological signals</article-title>
        <source>Sensors</source>
        <publisher-name>Multidisciplinary Digital Publishing Institute</publisher-name>
        <year iso-8601-date="2018">2018</year>
        <volume>18</volume>
        <issue>7</issue>
        <pub-id pub-id-type="doi">10.3390/s18072074</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-koelstra2011deap">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Koelstra</surname><given-names>Sander</given-names></name>
          <name><surname>Muhl</surname><given-names>Christian</given-names></name>
          <name><surname>Soleymani</surname><given-names>Mohammad</given-names></name>
          <name><surname>Lee</surname><given-names>Jong-Seok</given-names></name>
          <name><surname>Yazdani</surname><given-names>Ashkan</given-names></name>
          <name><surname>Ebrahimi</surname><given-names>Touradj</given-names></name>
          <name><surname>Pun</surname><given-names>Thierry</given-names></name>
          <name><surname>Nijholt</surname><given-names>Anton</given-names></name>
          <name><surname>Patras</surname><given-names>Ioannis</given-names></name>
        </person-group>
        <article-title>Deap: A database for emotion analysis; using physiological signals</article-title>
        <source>IEEE transactions on affective computing</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2011">2011</year>
        <volume>3</volume>
        <issue>1</issue>
        <pub-id pub-id-type="doi">10.1109/T-AFFC.2011.15</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Saffaryazdi2021">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Nastaran Saffaryazdi</surname><given-names>Suranga Nanayakkara</given-names><suffix>Syed Talal Wasim</suffix></name>
          <name><surname>Billinghurst</surname><given-names>Mark</given-names></name>
        </person-group>
        <article-title>Investigation of the use of facial micro-expressions in combination with EEG and physiological signals for emotion recognition</article-title>
        <publisher-name>Manuscript is under submission</publisher-name>
      </element-citation>
    </ref>
    <ref id="ref-Saffaryazdi2021conv">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Nastaran Saffaryazdi</surname><given-names>Nafiseh Saffaryazdi</given-names><suffix>Yenushka Goonesekera</suffix></name>
          <name><surname>Hailemariam</surname><given-names>Nebiyou Daniel</given-names></name>
          <name><surname>Ebasa Girma Temesgen</surname><given-names>Suranga Nanayakkara</given-names></name>
          <name><surname>Broadbent</surname><given-names>Elizabeth</given-names></name>
          <name><surname>Billinghurst</surname><given-names>Mark</given-names></name>
        </person-group>
        <article-title>Emotion recognition in conversations using brain and physiological signals</article-title>
        <publisher-name>Manuscript submitted for publication</publisher-name>
      </element-citation>
    </ref>
    <ref id="ref-egger2019emotion">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Egger</surname><given-names>Maria</given-names></name>
          <name><surname>Ley</surname><given-names>Matthias</given-names></name>
          <name><surname>Hanke</surname><given-names>Sten</given-names></name>
        </person-group>
        <article-title>Emotion recognition from physiological signal analysis: A review</article-title>
        <source>Electronic Notes in Theoretical Computer Science</source>
        <publisher-name>Elsevier</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <volume>343</volume>
        <pub-id pub-id-type="doi">10.1016/j.entcs.2019.04.009</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-vanneste2020towards">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Vanneste</surname><given-names>Pieter</given-names></name>
          <name><surname>Raes</surname><given-names>Annelies</given-names></name>
          <name><surname>Morton</surname><given-names>Jessica</given-names></name>
          <name><surname>Bombeke</surname><given-names>Klaas</given-names></name>
          <name><surname>Van Acker</surname><given-names>Bram B</given-names></name>
          <name><surname>Larmuseau</surname><given-names>Charlotte</given-names></name>
          <name><surname>Depaepe</surname><given-names>Fien</given-names></name>
          <name><surname>Van den Noortgate</surname><given-names>Wim</given-names></name>
        </person-group>
        <article-title>Towards measuring cognitive load through multimodal physiological data</article-title>
        <source>Cognition, Technology &amp; Work</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <pub-id pub-id-type="doi">10.1007/s10111-020-00641-0</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-mangaroska2021exploring">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Mangaroska</surname><given-names>Katerina</given-names></name>
          <name><surname>Sharma</surname><given-names>Kshitij</given-names></name>
          <name><surname>Gašević</surname><given-names>Dragan</given-names></name>
          <name><surname>Giannakos</surname><given-names>Michail</given-names></name>
        </person-group>
        <article-title>Exploring students’ cognitive and affective states during problem solving through multimodal data: Lessons learned from a programming activity</article-title>
        <source>Journal of Computer Assisted Learning</source>
        <publisher-name>Wiley Online Library</publisher-name>
        <year iso-8601-date="2021">2021</year>
        <pub-id pub-id-type="doi">10.1111/jcal.12590</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-dzedzickis2020human">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Dzedzickis</surname><given-names>Andrius</given-names></name>
          <name><surname>Kaklauskas</surname><given-names>Artūras</given-names></name>
          <name><surname>Bucinskas</surname><given-names>Vytautas</given-names></name>
        </person-group>
        <article-title>Human emotion recognition: Review of sensors and methods</article-title>
        <source>Sensors</source>
        <publisher-name>Multidisciplinary Digital Publishing Institute</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <volume>20</volume>
        <issue>3</issue>
        <pub-id pub-id-type="doi">10.3390/s20030592</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-seneviratne2017survey">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Seneviratne</surname><given-names>Suranga</given-names></name>
          <name><surname>Hu</surname><given-names>Yining</given-names></name>
          <name><surname>Nguyen</surname><given-names>Tham</given-names></name>
          <name><surname>Lan</surname><given-names>Guohao</given-names></name>
          <name><surname>Khalifa</surname><given-names>Sara</given-names></name>
          <name><surname>Thilakarathna</surname><given-names>Kanchana</given-names></name>
          <name><surname>Hassan</surname><given-names>Mahbub</given-names></name>
          <name><surname>Seneviratne</surname><given-names>Aruna</given-names></name>
        </person-group>
        <article-title>A survey of wearable devices and challenges</article-title>
        <source>IEEE Communications Surveys &amp; Tutorials</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2017">2017</year>
        <volume>19</volume>
        <issue>4</issue>
        <pub-id pub-id-type="doi">10.1109/COMST.2017.2731979</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
